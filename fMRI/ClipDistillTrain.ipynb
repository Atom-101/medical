{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "\n",
    "import kornia\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "\n",
    "\n",
    "import webdataset as wds\n",
    "from info_nce import InfoNCE\n",
    "import clip\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sd = torch.load('checkpoints/clip_image_vitB_large_768bs_subj01_best.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'train_losses', 'val_losses', 'train_topk', 'val_topk', 'lrs'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv.0.weight', 'conv.0.bias', 'lins.0.1.weight', 'lins.0.1.bias', 'lins.0.3.weight', 'lins.0.3.bias', 'lins.0.3.running_mean', 'lins.0.3.running_var', 'lins.0.3.num_batches_tracked', 'lins.0.5.weight', 'lins.0.5.bias', 'lins.0.7.weight', 'lins.0.7.bias', 'lins.0.7.running_mean', 'lins.0.7.running_var', 'lins.0.7.num_batches_tracked', 'lins.1.1.weight', 'lins.1.1.bias', 'lins.1.3.weight', 'lins.1.3.bias', 'lins.1.3.running_mean', 'lins.1.3.running_var', 'lins.1.3.num_batches_tracked', 'lins.1.5.weight', 'lins.1.5.bias', 'lins.1.7.weight', 'lins.1.7.bias', 'lins.1.7.running_mean', 'lins.1.7.running_var', 'lins.1.7.num_batches_tracked', 'lins.2.1.weight', 'lins.2.1.bias', 'lins.2.3.weight', 'lins.2.3.bias', 'lins.2.3.running_mean', 'lins.2.3.running_var', 'lins.2.3.num_batches_tracked', 'lins.2.5.weight', 'lins.2.5.bias', 'lins.2.7.weight', 'lins.2.7.bias', 'lins.2.7.running_mean', 'lins.2.7.running_var', 'lins.2.7.num_batches_tracked', 'lins.3.1.weight', 'lins.3.1.bias', 'lins.3.3.weight', 'lins.3.3.bias', 'lins.3.3.running_mean', 'lins.3.3.running_var', 'lins.3.3.num_batches_tracked', 'lins.3.5.weight', 'lins.3.5.bias', 'lins.3.7.weight', 'lins.3.7.bias', 'lins.3.7.running_mean', 'lins.3.7.running_var', 'lins.3.7.num_batches_tracked', 'lins.4.1.weight', 'lins.4.1.bias', 'lins.4.3.weight', 'lins.4.3.bias', 'lins.4.3.running_mean', 'lins.4.3.running_var', 'lins.4.3.num_batches_tracked', 'lins.4.5.weight', 'lins.4.5.bias', 'lins.4.7.weight', 'lins.4.7.bias', 'lins.4.7.running_mean', 'lins.4.7.running_var', 'lins.4.7.num_batches_tracked', 'lin1.weight', 'lin1.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3976, -0.4134, -0.4364,  ...,  0.3736, -0.4621,  0.4127])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['model_state_dict']['lins.0.7.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0140), tensor(0.4496))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['model_state_dict']['lins.0.7.weight'].mean(), sd['model_state_dict']['lins.0.7.weight'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6824, -0.7241, -0.5594,  ..., -0.6872, -0.8073, -0.7000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['model_state_dict']['lins.4.7.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_training: True\n",
      "Using model: clip_image_vitB\n",
      "ViT-L/14 cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 890M/890M [00:17<00:00, 53.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_var = images\n"
     ]
    }
   ],
   "source": [
    "# if full_training is True, use large batches and the entire training dataset \n",
    "full_training = True\n",
    "# image augmentation just for the CLIP image model that will be more semantic-focused\n",
    "train_augs = AugmentationSequential(\n",
    "    kornia.augmentation.RandomCrop((140, 140), p=0.3),\n",
    "    kornia.augmentation.Resize((224, 224)),\n",
    "    kornia.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "    kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "    kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "    data_keys=[\"input\"],\n",
    "    # random_apply = (1,4)\n",
    ")\n",
    "\n",
    "print('full_training:',full_training)\n",
    "\n",
    "model_name = 'clip_image_vitB' # CLIP ViT-L/14 image embeddings\n",
    "print(f\"Using model: {model_name}\")\n",
    "    \n",
    "if \"resnet\" in model_name: \n",
    "    clip_extractor = Clipper(\"RN50\")\n",
    "else:\n",
    "    clip_extractor = Clipper(\"ViT-L/14\", train_transforms=train_augs)\n",
    "    \n",
    "if \"text\" in model_name:     \n",
    "    image_var = 'trial' \n",
    "else:\n",
    "    image_var = 'images'\n",
    "print(\"image_var =\", image_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices 0\n",
      "num_workers 0\n",
      "batch_size 300\n",
      "global_batch_size 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,global_batch_size)\n\u001b[1;32m     34\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(num_samples \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m---> 35\u001b[0m num_worker_batches \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(\u001b[43mnum_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_worker_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_worker_batches)\n\u001b[1;32m     37\u001b[0m train_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnsd_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train/train_subj01_\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m0..49\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "nsd_path = '../../naturalscenesdataset/webdataset/'\n",
    "\n",
    "if not full_training: \n",
    "    num_devices = 1\n",
    "    num_workers = 1\n",
    "    print(\"num_workers\",num_workers)\n",
    "    batch_size = 16\n",
    "    print(\"batch_size\",batch_size)\n",
    "    num_samples = 500 \n",
    "    global_batch_size = batch_size * num_devices\n",
    "    print(\"global_batch_size\",global_batch_size)\n",
    "    num_batches = math.floor(num_samples / global_batch_size)\n",
    "    num_worker_batches = math.floor(num_batches / num_workers)\n",
    "    print(\"num_worker_batches\",num_worker_batches)\n",
    "    train_url = f\"{nsd_path}/train/train_subj01_{{0..1}}.tar\"\n",
    "else:\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"num_devices\",num_devices)\n",
    "    num_workers = num_devices\n",
    "    print(\"num_workers\",num_workers)\n",
    "    batch_size = 300\n",
    "    print(\"batch_size\",batch_size)\n",
    "    num_samples = 24983 # see metadata.json in webdataset_split folder\n",
    "    global_batch_size = batch_size * num_devices\n",
    "    print(\"global_batch_size\",global_batch_size)\n",
    "    num_batches = math.floor(num_samples / batch_size)\n",
    "    num_worker_batches = math.floor(num_batches / num_workers)\n",
    "    print(\"num_worker_batches\",num_worker_batches)\n",
    "    train_url = f\"{nsd_path}/train/train_subj01_{{0..49}}.tar\"\n",
    "\n",
    "train_data = wds.DataPipeline([wds.ResampledShards(train_url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.shuffle(500,initial=500),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", embs=\"sgxl_emb.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", image_var),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "train_dl = wds.WebLoader(train_data, num_workers=num_workers,\n",
    "                         batch_size=None, shuffle=False, persistent_workers=True)\n",
    "\n",
    "# Validation #\n",
    "num_samples = 492\n",
    "num_batches = math.ceil(num_samples / global_batch_size)\n",
    "num_worker_batches = math.ceil(num_batches / num_workers)\n",
    "print(\"validation: num_worker_batches\",num_worker_batches)\n",
    "\n",
    "url = f\"{nsd_path}/val/val_subj01_0.tar\"\n",
    "val_data = wds.DataPipeline([wds.ResampledShards(url),\n",
    "                    wds.tarfile_to_samples(),\n",
    "                    wds.decode(\"torch\"),\n",
    "                    wds.rename(images=\"jpg;png\", voxels=\"nsdgeneral.npy\", \n",
    "                                embs=\"sgxl_emb.npy\", trial=\"trial.npy\"),\n",
    "                    wds.to_tuple(\"voxels\", image_var),\n",
    "                    wds.batched(batch_size, partial=True),\n",
    "                ]).with_epoch(num_worker_batches)\n",
    "val_dl = wds.WebLoader(val_data, num_workers=num_workers,\n",
    "                       batch_size=None, shuffle=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check that your data loaders are working\u001b[39;00m\n\u001b[1;32m      2\u001b[0m out_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_i, (voxel, img_input) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dl\u001b[49m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_i)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxel.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,voxel\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "# check that your data loaders are working\n",
    "out_dim = 512\n",
    "for train_i, (voxel, img_input) in enumerate(train_dl):\n",
    "    print(\"idx\",train_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    if \"text\" in model_name:\n",
    "        emb = clip_extractor.embed_curated_annotations(subj01_annots[img_input])\n",
    "    else:\n",
    "        emb = clip_extractor.embed_image(img_input)\n",
    "    print(\"emb.shape\",emb.shape)\n",
    "    out_dim = emb.shape[1]\n",
    "    print(\"out_dim\", out_dim)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_devices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(brain_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m sched \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mOneCycleLR(opt, max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, \n\u001b[0;32m----> 5\u001b[0m                                             total_steps\u001b[38;5;241m=\u001b[39mEPOCHS\u001b[38;5;241m*\u001b[39m((\u001b[38;5;241m24983\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m300\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[43mnum_devices\u001b[49m), \n\u001b[1;32m      6\u001b[0m                                             final_div_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                             last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, pct_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mEPOCHS)\n\u001b[1;32m      9\u001b[0m nce \u001b[38;5;241m=\u001b[39m InfoNCE()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_devices' is not defined"
     ]
    }
   ],
   "source": [
    "brain_net = BrainNetwork(out_dim=512)\n",
    "EPOCHS = 100\n",
    "opt = torch.optim.AdamW(brain_net.parameters(), lr=1e-3)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=3e-4, \n",
    "                                            total_steps=EPOCHS*((24983//300)//num_devices), \n",
    "                                            final_div_factor=1000,\n",
    "                                            last_epoch=-1, pct_start=2/EPOCHS)\n",
    "\n",
    "nce = InfoNCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133747200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.numel(torch.nn.utils.parameters_to_vector(brain_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainNetwork(\n",
       "  (conv): Sequential(\n",
       "    (0): Linear(in_features=15742, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (lin): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.15, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.15, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.15, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.15, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lin1): Linear(in_features=4096, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "\n",
    "train_losses = []; val_losses = []\n",
    "train_topk = []; val_topk = []\n",
    "lrs = []\n",
    "epoch_logs = []\n",
    "\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(f\"num_epochs:{num_epochs} batch_size:{batch_size} lr:{initial_learning_rate}\")\n",
    "\n",
    "if full_training:\n",
    "    print(f\"Will be saving model checkpoints to checkpoints/{model_name}_subj01_epoch#.pth\")\n",
    "else:\n",
    "    print(f\"Warning: not saving model checkpoints\")\n",
    "\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "    \n",
    "pbar = tqdm(range(epoch, 100), ncols=250)\n",
    "for epoch in pbar:\n",
    "    brain_net.train()\n",
    "    similarities = []\n",
    "    for train_i, (voxel, img_input) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        voxel = voxel.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.bo_grad():\n",
    "                if image_var=='images': # using images\n",
    "                    emb = clip_extractor.embed_image(img_input)\n",
    "                else: # using text captions of the images \n",
    "                    emb = clip_extractor.embed_curated_annotations(subj01_annots[img_input])\n",
    "\n",
    "        emb = emb.float() # cast to float32\n",
    "        emb_ = brain_net(voxel)\n",
    "            \n",
    "        if torch.any(torch.isnan(emb_)):\n",
    "            raise ValueError(\"NaN found...\")\n",
    "                \n",
    "        emb_ = nn.functional.normalize(emb_,dim=-1) # l2 normalization on the embeddings\n",
    "            \n",
    "        labels = torch.arange(len(emb)).to(device)\n",
    "        loss_nce = nce(emb_.reshape(len(emb),-1),emb.reshape(len(emb),-1))\n",
    "        loss_soft = soft_clip_loss(emb_.reshape(len(emb),-1),emb.reshape(len(emb),-1))\n",
    "        loss = loss_nce + loss_soft\n",
    "            \n",
    "        similarities = batchwise_cosine_similarity(emb,emb_)\n",
    "\n",
    "        percent_correct = topk(similarities,labels,k=1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_topk.append(percent_correct.item())\n",
    "        \n",
    "    brain_net.eval()    \n",
    "    for val_i, (val_voxel, val_img_input) in enumerate(val_dl):\n",
    "        with torch.no_grad(): \n",
    "            val_voxel = val_voxel.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                if image_var=='images': # using images\n",
    "                    val_emb = clip_extractor.embed_image(val_img_input)\n",
    "                else: # using text captions of the images \n",
    "                    val_emb = clip_extractor.embed_curated_annotations(subj01_annots[val_img_input])\n",
    "\n",
    "                val_emb_ = brain_net(val_voxel)\n",
    "                val_emb_ = nn.functional.normalize(val_emb_,dim=-1) # l2 normalization on the embeddings\n",
    "            \n",
    "                labels = torch.arange(len(val_emb)).to(device)\n",
    "\n",
    "                val_loss = nce(val_emb_.reshape(len(val_emb),-1),val_emb.reshape(len(val_emb),-1))\n",
    "\n",
    "                val_similarities = batchwise_cosine_similarity(val_emb,val_emb_)\n",
    "\n",
    "                val_percent_correct = topk(val_similarities,labels,k=1)\n",
    "                \n",
    "            val_losses.append(val_loss.item())\n",
    "            val_topk.append(val_percent_correct.item())\n",
    "                \n",
    "    if epoch%5==4 and full_training:\n",
    "        print(f'saving checkpoints/{model_name}_subj01_epoch{epoch+1}.pth...')\n",
    "        if (using_ddp==False) or (using_ddp==True and local_rank==0):\n",
    "            state_dict = brain_net.state_dict()\n",
    "            if using_ddp: # if using DDP, convert DDP to non-DDP before saving\n",
    "                state_dict = brain_net.module.state_dict()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': state_dict,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'train_topk': train_topk,\n",
    "                'val_topk': val_topk,\n",
    "                'lrs': lrs,\n",
    "                }, f'checkpoints/{model_name}_subj01_epoch{epoch}.pth')\n",
    "        if using_ddp:\n",
    "            dist.barrier() # this tells the other gpus wait for the first gpu to finish saving the model\n",
    "            \n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # logging the average results across batches for current epoch\n",
    "    logs = OrderedDict(\n",
    "        loss=np.mean(train_losses[-(train_i+1):]),\n",
    "        topk=np.mean(train_topk[-(train_i+1):]),\n",
    "        val_loss=np.mean(val_losses[-(val_i+1):]),\n",
    "        val_topk=np.mean(val_topk[-(val_i+1):]),\n",
    "        lr=lrs[-1],\n",
    "    )\n",
    "    pbar.set_postfix(**logs)\n",
    "    epoch_logs.append(logs)\n",
    "    if full_training:\n",
    "        pd.DataFrame(epoch_logs).to_csv(f'checkpoints/{model_name}_subj01.epoch-logs.csv')\n",
    "    \n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snemi_new",
   "language": "python",
   "name": "snemi_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
